{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57236,"databundleVersionId":7292407,"sourceType":"competition"},{"sourceId":7348254,"sourceType":"datasetVersion","datasetId":4266997},{"sourceId":7406639,"sourceType":"datasetVersion","datasetId":4307493}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is a slightly improved version of \"[Enefit PEBOP: EDA (Plotly) and Modelling](https://www.kaggle.com/code/siddhvr/enefit-pebop-eda-plotly-and-modelling?scriptVersionId=158742203)\" notebook, namely 3-rd version that scores best on public LB. List of changes:\n- Trained models are now loaded from an external [dataset](https://www.kaggle.com/datasets/kononenko/v2-enefit-pebop-eda-plotly-and-modelling/data), instead of a notebook. This fixes issues in the case a new notebook version is created;\n- Ensemble weights are tuned to maximize LB score.\n\nIf you find this notebook useful, please appreciate the original work by @siddhvr.","metadata":{}},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport gc\nimport pickle\nimport datetime\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport plotly.express as px\nimport joblib\nfrom sklearn.ensemble import VotingRegressor\nimport lightgbm as lgb\nfrom joblib import load\n\nimport holidays","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:30:40.595861Z","iopub.execute_input":"2024-01-16T08:30:40.596309Z","iopub.status.idle":"2024-01-16T08:30:43.99279Z","shell.execute_reply.started":"2024-01-16T08:30:40.596267Z","shell.execute_reply":"2024-01-16T08:30:43.991129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <h4> This is a bonus version of this Notebook, since I have exhausted my entire GPU quota for the week ðŸ™ƒ. \nHere, I have tried to ensemble the predictions of two trained models. But the catch is that both were trained on datasets having different number of features. I have tried to explore the implications of this difference on the end predictions. ","metadata":{}},{"cell_type":"markdown","source":"> <h4> I will be using a trained model which I published earlier as a dataset. It had an independent MAE of 65.51 on the public test set (LeaderBoard) and the other model will be the one which we trained on Version 1[(see here)](https://www.kaggle.com/code/siddhvr/enefit-pebop-eda-plotly-and-modelling) of this notebook.","metadata":{}},{"cell_type":"markdown","source":"### Step 1 : Create a class to access and process all the data files","metadata":{}},{"cell_type":"code","source":"class DataStorage:\n    root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n\n    data_cols = [\n        \"target\",\n        \"county\",\n        \"is_business\",\n        \"product_type\",\n        \"is_consumption\",\n        \"datetime\",\n        \"row_id\",\n    ]\n    client_cols = [\n        \"product_type\",\n        \"county\",\n        \"eic_count\",\n        \"installed_capacity\",\n        \"is_business\",\n        \"date\",\n    ]\n    gas_prices_cols = [\"forecast_date\", \"lowest_price_per_mwh\", \"highest_price_per_mwh\"]\n    electricity_prices_cols = [\"forecast_date\", \"euros_per_mwh\"]\n    forecast_weather_cols = [\n        \"latitude\",\n        \"longitude\",\n        \"hours_ahead\",\n        \"temperature\",\n        \"dewpoint\",\n        \"cloudcover_high\",\n        \"cloudcover_low\",\n        \"cloudcover_mid\",\n        \"cloudcover_total\",\n        \"10_metre_u_wind_component\",\n        \"10_metre_v_wind_component\",\n        \"forecast_datetime\",\n        \"direct_solar_radiation\",\n        \"surface_solar_radiation_downwards\",\n        \"snowfall\",\n        \"total_precipitation\",\n    ]\n    historical_weather_cols = [\n        \"datetime\",\n        \"temperature\",\n        \"dewpoint\",\n        \"rain\",\n        \"snowfall\",\n        \"surface_pressure\",\n        \"cloudcover_total\",\n        \"cloudcover_low\",\n        \"cloudcover_mid\",\n        \"cloudcover_high\",\n        \"windspeed_10m\",\n        \"winddirection_10m\",\n        \"shortwave_radiation\",\n        \"direct_solar_radiation\",\n        \"diffuse_radiation\",\n        \"latitude\",\n        \"longitude\",\n    ]\n    location_cols = [\"longitude\", \"latitude\", \"county\"]\n    target_cols = [\n        \"target\",\n        \"county\",\n        \"is_business\",\n        \"product_type\",\n        \"is_consumption\",\n        \"datetime\",\n    ]\n\n    def __init__(self):\n        self.df_data = pl.read_csv(\n            os.path.join(self.root, \"train.csv\"),\n            columns=self.data_cols,\n            try_parse_dates=True,\n        )\n        self.df_client = pl.read_csv(\n            os.path.join(self.root, \"client.csv\"),\n            columns=self.client_cols,\n            try_parse_dates=True,\n        )\n        self.df_gas_prices = pl.read_csv(\n            os.path.join(self.root, \"gas_prices.csv\"),\n            columns=self.gas_prices_cols,\n            try_parse_dates=True,\n        )\n        self.df_electricity_prices = pl.read_csv(\n            os.path.join(self.root, \"electricity_prices.csv\"),\n            columns=self.electricity_prices_cols,\n            try_parse_dates=True,\n        )\n        self.df_forecast_weather = pl.read_csv(\n            os.path.join(self.root, \"forecast_weather.csv\"),\n            columns=self.forecast_weather_cols,\n            try_parse_dates=True,\n        )\n        self.df_historical_weather = pl.read_csv(\n            os.path.join(self.root, \"historical_weather.csv\"),\n            columns=self.historical_weather_cols,\n            try_parse_dates=True,\n        )\n        self.df_weather_station_to_county_mapping = pl.read_csv(\n            os.path.join(self.root, \"weather_station_to_county_mapping.csv\"),\n            columns=self.location_cols,\n            try_parse_dates=True,\n        )\n        self.df_data = self.df_data.filter(\n            pl.col(\"datetime\") >= pd.to_datetime(\"2022-01-01\")\n        )\n        self.df_target = self.df_data.select(self.target_cols)\n\n        self.schema_data = self.df_data.schema\n        self.schema_client = self.df_client.schema\n        self.schema_gas_prices = self.df_gas_prices.schema\n        self.schema_electricity_prices = self.df_electricity_prices.schema\n        self.schema_forecast_weather = self.df_forecast_weather.schema\n        self.schema_historical_weather = self.df_historical_weather.schema\n        self.schema_target = self.df_target.schema\n\n        self.df_weather_station_to_county_mapping = (\n            self.df_weather_station_to_county_mapping.with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            )\n        )\n\n    def update_with_new_data(\n        self,\n        df_new_client,\n        df_new_gas_prices,\n        df_new_electricity_prices,\n        df_new_forecast_weather,\n        df_new_historical_weather,\n        df_new_target,\n    ):\n        df_new_client = pl.from_pandas(\n            df_new_client[self.client_cols], schema_overrides=self.schema_client\n        )\n        df_new_gas_prices = pl.from_pandas(\n            df_new_gas_prices[self.gas_prices_cols],\n            schema_overrides=self.schema_gas_prices,\n        )\n        df_new_electricity_prices = pl.from_pandas(\n            df_new_electricity_prices[self.electricity_prices_cols],\n            schema_overrides=self.schema_electricity_prices,\n        )\n        df_new_forecast_weather = pl.from_pandas(\n            df_new_forecast_weather[self.forecast_weather_cols],\n            schema_overrides=self.schema_forecast_weather,\n        )\n        df_new_historical_weather = pl.from_pandas(\n            df_new_historical_weather[self.historical_weather_cols],\n            schema_overrides=self.schema_historical_weather,\n        )\n        df_new_target = pl.from_pandas(\n            df_new_target[self.target_cols], schema_overrides=self.schema_target\n        )\n\n        self.df_client = pl.concat([self.df_client, df_new_client]).unique(\n            [\"date\", \"county\", \"is_business\", \"product_type\"]\n        )\n        self.df_gas_prices = pl.concat([self.df_gas_prices, df_new_gas_prices]).unique(\n            [\"forecast_date\"]\n        )\n        self.df_electricity_prices = pl.concat(\n            [self.df_electricity_prices, df_new_electricity_prices]\n        ).unique([\"forecast_date\"])\n        self.df_forecast_weather = pl.concat(\n            [self.df_forecast_weather, df_new_forecast_weather]\n        ).unique([\"forecast_datetime\", \"latitude\", \"longitude\", \"hours_ahead\"])\n        self.df_historical_weather = pl.concat(\n            [self.df_historical_weather, df_new_historical_weather]\n        ).unique([\"datetime\", \"latitude\", \"longitude\"])\n        self.df_target = pl.concat([self.df_target, df_new_target]).unique(\n            [\"datetime\", \"county\", \"is_business\", \"product_type\", \"is_consumption\"]\n        )\n\n    def preprocess_test(self, df_test):\n        df_test = df_test.rename(columns={\"prediction_datetime\": \"datetime\"})\n        df_test = pl.from_pandas(\n            df_test[self.data_cols[1:]], schema_overrides=self.schema_data\n        )\n        return df_test\n","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:30:43.994668Z","iopub.execute_input":"2024-01-16T08:30:43.995803Z","iopub.status.idle":"2024-01-16T08:30:44.017058Z","shell.execute_reply.started":"2024-01-16T08:30:43.995741Z","shell.execute_reply":"2024-01-16T08:30:44.014875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 2: Create Feature Enegineering Classes","metadata":{}},{"cell_type":"markdown","source":"> <h4> For ease of understanding and clarity, I will be using two different feature engineering classes for both models. However, we can also merge them and make a single class since, most of the functions are the same with minor changes elsewhere. But remember to have two different <i>'generate_features'</i> functions for both of these in that case.","metadata":{}},{"cell_type":"code","source":"class FeatureEngineer:\n    def __init__(self, data):\n        self.data = data\n        self.estonian_holidays = list(\n            holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n        )\n\n    def _general_features(self, df_features):\n        df_features = (\n            df_features.with_columns(\n                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n                pl.col(\"datetime\").dt.day().alias(\"day\"),pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n                pl.col(\"datetime\").dt.month().alias(\"month\"),pl.col(\"datetime\").dt.year().alias(\"year\"),\n            )\n            .with_columns(pl.concat_str(\"county\",\"is_business\",\"product_type\",\"is_consumption\",separator=\"_\",).alias(\"segment\"),)\n            .with_columns(\n                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),(np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),(np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n            )\n        )\n        return df_features\n\n    def _client_features(self, df_features):\n        df_client = self.data.df_client\n\n        df_features = df_features.join(\n            df_client.with_columns((pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n            ),on=[\"county\", \"is_business\", \"product_type\", \"date\"],how=\"left\",\n        )\n        return df_features\n    \n    def is_country_holiday(self, row):\n        return (\n            datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])\n            in self.estonian_holidays\n        )\n\n    def _holidays_features(self, df_features):\n        df_features = df_features.with_columns(\n            pl.struct([\"year\", \"month\", \"day\"])\n            .apply(self.is_country_holiday)\n            .alias(\"is_country_holiday\")\n        )\n        return df_features\n\n    def _forecast_weather_features(self, df_features):\n        df_forecast_weather = self.data.df_forecast_weather\n        df_weather_station_to_county_mapping = (\n            self.data.df_weather_station_to_county_mapping\n        )\n\n        df_forecast_weather = (\n            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n#             .drop(\"hours_ahead\")\n            .with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),)\n            .join(df_weather_station_to_county_mapping,how=\"left\",on=[\"longitude\", \"latitude\"],).drop(\"longitude\", \"latitude\"))\n\n        df_forecast_weather_date = (df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\"))\n\n        df_forecast_weather_local = (df_forecast_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean())\n\n        for hours_lag in [0, 7 * 24]:\n            df_features = df_features.join(\n                df_forecast_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),on=\"datetime\",how=\"left\",suffix=f\"_forecast_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_forecast_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),on=[\"county\", \"datetime\"],how=\"left\",suffix=f\"_forecast_local_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _historical_weather_features(self, df_features):\n        df_historical_weather = self.data.df_historical_weather\n        df_weather_station_to_county_mapping = (self.data.df_weather_station_to_county_mapping)\n\n        df_historical_weather = (\n            df_historical_weather.with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            ).join(df_weather_station_to_county_mapping,how=\"left\",on=[\"longitude\", \"latitude\"],).drop(\"longitude\", \"latitude\")\n        )\n\n        df_historical_weather_date = (df_historical_weather.group_by(\"datetime\").mean().drop(\"county\"))\n\n        df_historical_weather_local = (\n            df_historical_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean()\n        )\n\n        for hours_lag in [2 * 24, 7 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),on=\"datetime\",how=\"left\",suffix=f\"_historical_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_historical_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),on=[\"county\", \"datetime\"],how=\"left\",suffix=f\"_historical_local_{hours_lag}h\",\n            )\n\n        for hours_lag in [1 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),).filter(pl.col(\"hour\") <= 10).drop(\"hour\"),on=\"datetime\",how=\"left\",\n                suffix=f\"_historical_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _target_features(self, df_features):\n        df_target = self.data.df_target\n\n        df_target_all_type_sum = (df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\"))\n\n        df_target_all_county_type_sum = (df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\", \"county\"))\n        \n        hours_list=[i*24 for i in range(2,15)]\n\n        for hours_lag in hours_list:\n            df_features = df_features.join(\n                df_target.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n                on=[\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",],\n                how=\"left\",\n            )\n\n        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n            df_features = df_features.join(\n                df_target_all_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],how=\"left\",\n            )\n\n            df_features = df_features.join(\n                df_target_all_county_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n                on=[\"is_business\", \"is_consumption\", \"datetime\"],how=\"left\",\n                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n            )\n\n        cols_for_stats = [f\"target_{hours_lag}h\" for hours_lag in hours_list[:4]]\n        \n        df_features = df_features.with_columns(\n            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n            df_features.select(cols_for_stats).transpose().std().transpose().to_series().alias(f\"target_std\"),\n            )\n\n        for target_prefix, lag_nominator, lag_denomonator in [\n            (\"target\", 24 * 7, 24 * 14),(\"target\", 24 * 2, 24 * 9),(\"target\", 24 * 3, 24 * 10),(\"target\", 24 * 2, 24 * 3),\n            (\"target_all_type_sum\", 24 * 2, 24 * 3),(\"target_all_type_sum\", 24 * 7, 24 * 14),\n            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),(\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n        ]:\n            df_features = df_features.with_columns(\n                (pl.col(f\"{target_prefix}_{lag_nominator}h\")/ (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n            )\n\n        return df_features\n\n    def _reduce_memory_usage(self, df_features):\n        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n        return df_features\n\n    def _drop_columns(self, df_features):\n        df_features = df_features.drop(\"datetime\", \"hour\", \"dayofyear\")\n        return df_features\n\n    def _to_pandas(self, df_features, y):\n        cat_cols = [\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"segment\",]\n\n        if y is not None:\n            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n        else:\n            df_features = df_features.to_pandas()\n\n        df_features = df_features.set_index(\"row_id\")\n        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n\n        return df_features\n    \n    # added some new features here\n    def _additional_features(self,df):\n        for col in [\n                    'temperature', \n                    'dewpoint', \n                    '10_metre_u_wind_component', \n                    '10_metre_v_wind_component', \n            ]:\n            for window in [1]:\n                df[f\"{col}_diff_{window}\"] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])[col].diff(window)\n        return df\n    \n    def _log_outliers(self,df):\n        l1=['installed_capacity', 'target_mean', 'target_std']\n        for i in l1:\n            df = df.with_columns([(f\"log_{i}\", pl.when(df[i] != 0).then(np.log(pl.col(i))).otherwise(0))])\n        return df\n        \n\n    def generate_features(self, df_prediction_items,isTrain):\n        if \"target\" in df_prediction_items.columns:\n            df_prediction_items, y = (\n                df_prediction_items.drop(\"target\"),\n                df_prediction_items.select(\"target\"),\n            )\n        else:\n            y = None\n\n        df_features = df_prediction_items.with_columns(\n            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n        )\n\n        for add_features in [\n            self._general_features,self._client_features,self._forecast_weather_features,\n            self._historical_weather_features,self._target_features,self._holidays_features,\n            self._log_outliers,self._reduce_memory_usage,self._drop_columns,]:\n            df_features = add_features(df_features)\n\n        df_features = self._to_pandas(df_features, y)\n        df_features = self._additional_features(df_features)\n\n        return df_features\n","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:30:44.019082Z","iopub.execute_input":"2024-01-16T08:30:44.019561Z","iopub.status.idle":"2024-01-16T08:30:44.061237Z","shell.execute_reply.started":"2024-01-16T08:30:44.019526Z","shell.execute_reply":"2024-01-16T08:30:44.059687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeaturesGenerator:\n    def __init__(self, data_storage):\n        self.data_storage = data_storage\n        self.estonian_holidays = list(\n            holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n        )\n\n    def _add_general_features(self, df_features):\n        df_features = (\n            df_features.with_columns(\n                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n                pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n                pl.col(\"datetime\").dt.day().alias(\"day\"),\n                pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n                pl.col(\"datetime\").dt.month().alias(\"month\"),\n                pl.col(\"datetime\").dt.year().alias(\"year\"),\n            )\n            .with_columns(\n                pl.concat_str(\n                    \"county\",\n                    \"is_business\",\n                    \"product_type\",\n                    \"is_consumption\",\n                    separator=\"_\",\n                ).alias(\"segment\"),\n            )\n            .with_columns(\n                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n                (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n                (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n            )\n        )\n        return df_features\n\n    def _add_client_features(self, df_features):\n        df_client = self.data_storage.df_client\n\n        df_features = df_features.join(\n            df_client.with_columns(\n                (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n            ),\n            on=[\"county\", \"is_business\", \"product_type\", \"date\"],\n            how=\"left\",\n        )\n        return df_features\n    \n    def is_country_holiday(self, row):\n        return (\n            datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])\n            in self.estonian_holidays\n        )\n\n    def _add_holidays_features(self, df_features):\n        df_features = df_features.with_columns(\n            pl.struct([\"year\", \"month\", \"day\"])\n            .apply(self.is_country_holiday)\n            .alias(\"is_country_holiday\")\n        )\n        return df_features\n\n    def _add_forecast_weather_features(self, df_features):\n        df_forecast_weather = self.data_storage.df_forecast_weather\n        df_weather_station_to_county_mapping = (\n            self.data_storage.df_weather_station_to_county_mapping\n        )\n\n        df_forecast_weather = (\n            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n            .drop(\"hours_ahead\")\n            .with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            )\n            .join(\n                df_weather_station_to_county_mapping,\n                how=\"left\",\n                on=[\"longitude\", \"latitude\"],\n            )\n            .drop(\"longitude\", \"latitude\")\n        )\n\n        df_forecast_weather_date = (\n            df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\")\n        )\n\n        df_forecast_weather_local = (\n            df_forecast_weather.filter(pl.col(\"county\").is_not_null())\n            .group_by(\"county\", \"datetime\")\n            .mean()\n        )\n\n        for hours_lag in [0, 7 * 24]:\n            df_features = df_features.join(\n                df_forecast_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_forecast_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_forecast_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=[\"county\", \"datetime\"],\n                how=\"left\",\n                suffix=f\"_forecast_local_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _add_historical_weather_features(self, df_features):\n        df_historical_weather = self.data_storage.df_historical_weather\n        df_weather_station_to_county_mapping = (\n            self.data_storage.df_weather_station_to_county_mapping\n        )\n\n        df_historical_weather = (\n            df_historical_weather.with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            )\n            .join(\n                df_weather_station_to_county_mapping,\n                how=\"left\",\n                on=[\"longitude\", \"latitude\"],\n            )\n            .drop(\"longitude\", \"latitude\")\n        )\n\n        df_historical_weather_date = (\n            df_historical_weather.group_by(\"datetime\").mean().drop(\"county\")\n        )\n\n        df_historical_weather_local = (\n            df_historical_weather.filter(pl.col(\"county\").is_not_null())\n            .group_by(\"county\", \"datetime\")\n            .mean()\n        )\n\n        for hours_lag in [2 * 24, 7 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_historical_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_historical_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=[\"county\", \"datetime\"],\n                how=\"left\",\n                suffix=f\"_historical_local_{hours_lag}h\",\n            )\n\n        for hours_lag in [1 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n                )\n                .filter(pl.col(\"hour\") <= 10)\n                .drop(\"hour\"),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_historical_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _add_target_features(self, df_features):\n        df_target = self.data_storage.df_target\n\n        df_target_all_type_sum = (\n            df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"])\n            .sum()\n            .drop(\"product_type\")\n        )\n\n        df_target_all_county_type_sum = (\n            df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"])\n            .sum()\n            .drop(\"product_type\", \"county\")\n        )\n\n        for hours_lag in [2 * 24,3 * 24,4 * 24,5 * 24,6 * 24,7 * 24,8 * 24,9 * 24,10 * 24,11 * 24,12 * 24,13 * 24,14 * 24,]:\n            df_features = df_features.join(\n                df_target.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n                on=[\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",],\n                how=\"left\",\n            )\n\n        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n            df_features = df_features.join(\n                df_target_all_type_sum.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],\n                how=\"left\",\n            )\n\n            df_features = df_features.join(\n                df_target_all_county_type_sum.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n                on=[\"is_business\", \"is_consumption\", \"datetime\"],\n                how=\"left\",\n                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n            )\n\n        cols_for_stats = [\n            f\"target_{hours_lag}h\" for hours_lag in [2 * 24, 3 * 24, 4 * 24, 5 * 24]\n        ]\n        df_features = df_features.with_columns(\n            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n            df_features.select(cols_for_stats)\n            .transpose()\n            .std()\n            .transpose()\n            .to_series()\n            .alias(f\"target_std\"),\n        )\n\n        for target_prefix, lag_nominator, lag_denomonator in [\n            (\"target\", 24 * 7, 24 * 14),\n            (\"target\", 24 * 2, 24 * 9),\n            (\"target\", 24 * 3, 24 * 10),\n            (\"target\", 24 * 2, 24 * 3),\n            (\"target_all_type_sum\", 24 * 2, 24 * 3),\n            (\"target_all_type_sum\", 24 * 7, 24 * 14),\n            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),\n            (\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n        ]:\n            df_features = df_features.with_columns(\n                (\n                    pl.col(f\"{target_prefix}_{lag_nominator}h\")\n                    / (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n            )\n\n        return df_features\n\n    def _reduce_memory_usage(self, df_features):\n        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n        return df_features\n\n    def _drop_columns(self, df_features):\n        df_features = df_features.drop(\n            \"date\", \"datetime\", \"hour\", \"dayofyear\"\n        )\n        return df_features\n\n    def _to_pandas(self, df_features, y):\n        cat_cols = [\n            \"county\",\n            \"is_business\",\n            \"product_type\",\n            \"is_consumption\",\n            \"segment\",\n        ]\n\n        if y is not None:\n            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n        else:\n            df_features = df_features.to_pandas()\n\n        df_features = df_features.set_index(\"row_id\")\n        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n\n        return df_features\n    \n    def generate_features(self, df_prediction_items):\n        if \"target\" in df_prediction_items.columns:\n            df_prediction_items, y = (\n                df_prediction_items.drop(\"target\"),\n                df_prediction_items.select(\"target\"),\n            )\n        else:\n            y = None\n\n        df_features = df_prediction_items.with_columns(\n            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n        )\n\n        for add_features in [\n            self._add_general_features,\n            self._add_client_features,\n            self._add_forecast_weather_features,\n            self._add_historical_weather_features,\n            self._add_target_features,\n            self._add_holidays_features,\n            self._reduce_memory_usage,\n            self._drop_columns,\n        ]:\n            df_features = add_features(df_features)\n\n        df_features = self._to_pandas(df_features, y)\n\n        return df_features","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:30:44.064651Z","iopub.execute_input":"2024-01-16T08:30:44.065344Z","iopub.status.idle":"2024-01-16T08:30:44.107122Z","shell.execute_reply.started":"2024-01-16T08:30:44.065302Z","shell.execute_reply":"2024-01-16T08:30:44.105631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialisation","metadata":{}},{"cell_type":"code","source":"data_storage = DataStorage()\nfeatures_generator = FeaturesGenerator(data_storage=data_storage)\nfeat_gen = FeatureEngineer(data=data_storage)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:30:44.109117Z","iopub.execute_input":"2024-01-16T08:30:44.109688Z","iopub.status.idle":"2024-01-16T08:30:48.622522Z","shell.execute_reply.started":"2024-01-16T08:30:44.109647Z","shell.execute_reply":"2024-01-16T08:30:48.621699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Generation","metadata":{}},{"cell_type":"markdown","source":"> <h5>Although there is no requirement as such to create the training datasets, (since we are not doing any model training here) I have created them only to show the difference in the features between the two datasets we are using here","metadata":{}},{"cell_type":"code","source":"df_train_features = features_generator.generate_features(data_storage.df_data)\ndf_train_features = df_train_features[df_train_features['target'].notnull()]","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:30:48.624133Z","iopub.execute_input":"2024-01-16T08:30:48.624809Z","iopub.status.idle":"2024-01-16T08:31:07.461958Z","shell.execute_reply.started":"2024-01-16T08:30:48.62477Z","shell.execute_reply":"2024-01-16T08:31:07.460622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = feat_gen.generate_features(data_storage.df_data,True)\ndf_train = df_train[df_train['target'].notnull()]","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:31:07.463747Z","iopub.execute_input":"2024-01-16T08:31:07.464205Z","iopub.status.idle":"2024-01-16T08:31:26.239654Z","shell.execute_reply.started":"2024-01-16T08:31:07.464158Z","shell.execute_reply":"2024-01-16T08:31:26.238207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_features.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:31:26.240861Z","iopub.execute_input":"2024-01-16T08:31:26.241178Z","iopub.status.idle":"2024-01-16T08:31:26.248531Z","shell.execute_reply.started":"2024-01-16T08:31:26.241148Z","shell.execute_reply":"2024-01-16T08:31:26.246848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:31:26.24986Z","iopub.execute_input":"2024-01-16T08:31:26.251407Z","iopub.status.idle":"2024-01-16T08:31:26.265736Z","shell.execute_reply.started":"2024-01-16T08:31:26.251301Z","shell.execute_reply":"2024-01-16T08:31:26.264487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:31:26.26979Z","iopub.execute_input":"2024-01-16T08:31:26.270108Z","iopub.status.idle":"2024-01-16T08:31:26.39516Z","shell.execute_reply.started":"2024-01-16T08:31:26.270081Z","shell.execute_reply":"2024-01-16T08:31:26.393659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in df_train.columns:\n    if (col not in df_train_features.columns):\n        print(col)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:31:26.396249Z","iopub.execute_input":"2024-01-16T08:31:26.396557Z","iopub.status.idle":"2024-01-16T08:31:26.406084Z","shell.execute_reply.started":"2024-01-16T08:31:26.396531Z","shell.execute_reply":"2024-01-16T08:31:26.40433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if('date' in df_train_features.columns):\n    df_train_features.drop(columns=['date'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:31:26.407124Z","iopub.execute_input":"2024-01-16T08:31:26.40781Z","iopub.status.idle":"2024-01-16T08:31:26.41717Z","shell.execute_reply.started":"2024-01-16T08:31:26.407775Z","shell.execute_reply":"2024-01-16T08:31:26.4159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"> We will be using Plotly for Data Analysis. \nPlotly has **hover tool capabilities** that allow us to detect any outliers or anomalies in a large number of data points.\nThe resultant plots are highly interactive and it allows to zoom in and focus on certain regions of the plot for a deeper analysis.\nIt allows for endless customization of graphs that makes the plot more meaningful and understandable.","metadata":{}},{"cell_type":"markdown","source":"## segment-wise energy consumption","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)\n\nsegment_list = df_train.segment.unique()[:3]\nc=['orangered','blue','gold']\ni=0\n# Filter the dataset for the specific segment\nfor seg in segment_list:\n    consumption_segment = df_train[df_train.segment == seg]\n\n    # Create a line plot using Plotly Express\n    fig = px.line(consumption_segment, x='date', y='target', \n              title=f'Target Over Time for Segment {seg}',\n              labels={'date': 'Date', 'target': 'Target'},\n              template='plotly_dark',line_shape='linear')\n    fig.update_traces(line=dict(color=c[i], width=1.5))\n\n    # Customize the x-axis date format and tick interval\n    fig.update_xaxes(type='date', tickformat='%Y-%m-%d', tickmode='linear', dtick=15)\n    i+=1\n    # Show the plot\n    fig.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:31:26.419147Z","iopub.execute_input":"2024-01-16T08:31:26.419429Z","iopub.status.idle":"2024-01-16T08:31:28.936707Z","shell.execute_reply.started":"2024-01-16T08:31:26.419402Z","shell.execute_reply":"2024-01-16T08:31:28.935202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FFT Analysis","metadata":{}},{"cell_type":"markdown","source":"> <h4>Initially studied it in this <a href=\"https://www.kaggle.com/code/chaozhuang/enefit-eda-w-fft-ssa-lgbm-voting-regressor\">amazing notebook</a> by <a href=\"https://www.kaggle.com/chaozhuang\">CHAO ZHUANG</a>. He has done a very deep analysis and explained everything on the go</h4>","metadata":{}},{"cell_type":"code","source":"import plotly.graph_objects as go\n\nsegment_list = df_train.segment.unique()[:10]\nexample_df = df_train[np.isin(df_train.segment, segment_list)]\nsegments = example_df['segment'].unique()\n\n# Define periods in days and calculate corresponding frequencies\nperiods = {'Annual': 365,'Semiannual': 365 / 2,'Quarterly': 365 / 4,'Monthly': 30,'Biweekly': 14,'Weekly': 7,'Semiweekly': 3.5}\nfrequencies_for_periods = {k: 1 / v for k, v in periods.items()}\n\n# Initialize the figure for the spectra using Plotly\nfig = go.Figure()\n\n# Convert the x-axis to a log scale\nfig.update_xaxes(type='log')\n\n# Plot the spectrum for each segment with offset\nfor i, segment in enumerate(segments):\n    segment_data = example_df[example_df['segment'] == segment]['target']\n    fft_values = np.fft.fft(segment_data)\n    frequencies = np.fft.fftfreq(len(fft_values), d=1)\n    magnitudes = np.abs(fft_values)[frequencies > 0]\n    normalized_magnitudes = magnitudes / np.max(magnitudes)\n    positive_freqs = frequencies[frequencies > 0]\n\n    # Filter out frequencies corresponding to periods longer than 'Semiannual'\n    valid_freqs = positive_freqs[positive_freqs > frequencies_for_periods['Semiannual']]\n    valid_magnitudes = normalized_magnitudes[positive_freqs > frequencies_for_periods['Semiannual']]\n\n    # Offset each segment's spectrum for clarity\n    offset_magnitudes = valid_magnitudes + i\n\n    fig.add_trace(go.Scatter(x=valid_freqs, y=offset_magnitudes, mode='lines', name=f'Segment {segment}'))\n\n# Customize the plot layout\nfig.update_layout(\n    title='Frequency Spectra of hourly target for Each Segment',\n    xaxis_title='Frequency',\n    yaxis_title='Normalized Magnitude + Offset',\n    xaxis=dict(tickvals=list(frequencies_for_periods.values()), ticktext=list(frequencies_for_periods.keys())),\n    showlegend=True\n)\n\n# Show the plot\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:31:28.937929Z","iopub.execute_input":"2024-01-16T08:31:28.938548Z","iopub.status.idle":"2024-01-16T08:31:29.645399Z","shell.execute_reply.started":"2024-01-16T08:31:28.938514Z","shell.execute_reply":"2024-01-16T08:31:29.644524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fft_plots_enefit(name):\n    # Initialize the figure for the spectrum using Plotly\n    fig = go.Figure()\n\n    # Convert the x-axis to a log scale\n    fig.update_xaxes(type='log')\n\n    # Plot the spectrum for the specified segment\n    segment_data = example_df[example_df['segment'] == '0_0_1_1'][name]\n    fft_values = np.fft.fft(segment_data)\n    frequencies = np.fft.fftfreq(len(fft_values), d=1)\n    magnitudes = np.abs(fft_values)[frequencies > 0]\n    positive_freqs = frequencies[frequencies > 0]\n\n    # Filter out frequencies corresponding to periods longer than 'Semiannual'\n    valid_freqs = positive_freqs[positive_freqs > frequencies_for_periods['Semiannual']]\n    valid_magnitudes = magnitudes[positive_freqs > frequencies_for_periods['Semiannual']]\n\n    fig.add_trace(go.Scatter(x=valid_freqs, y=valid_magnitudes, mode='lines', name='0_0_1_1'))\n\n    #  Customize the plot layout\n    fig.update_layout(\n    title=f'{name} frequency spectrum',\n    xaxis_title='Frequency',\n    yaxis_title='Magnitude',\n    xaxis=dict(tickvals=list(frequencies_for_periods.values()), ticktext=list(frequencies_for_periods.keys())),\n    showlegend=True,\n    )\n\n    # Show the plot\n    fig.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:31:29.646375Z","iopub.execute_input":"2024-01-16T08:31:29.6467Z","iopub.status.idle":"2024-01-16T08:31:29.656307Z","shell.execute_reply.started":"2024-01-16T08:31:29.646671Z","shell.execute_reply":"2024-01-16T08:31:29.65508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_list=['temperature','direct_solar_radiation']\nfor i in plot_list:\n    fft_plots_enefit(i)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:31:29.657586Z","iopub.execute_input":"2024-01-16T08:31:29.657884Z","iopub.status.idle":"2024-01-16T08:31:29.741776Z","shell.execute_reply.started":"2024-01-16T08:31:29.657857Z","shell.execute_reply":"2024-01-16T08:31:29.740017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Trained Models","metadata":{}},{"cell_type":"code","source":"c1 = load('/kaggle/input/enefit-trained-model/voting_regressor_consumption_model.joblib')\np1 = load('/kaggle/input/enefit-trained-model/voting_regressor_production_model.joblib')\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:31:29.743318Z","iopub.execute_input":"2024-01-16T08:31:29.743661Z","iopub.status.idle":"2024-01-16T08:32:27.033047Z","shell.execute_reply.started":"2024-01-16T08:31:29.74363Z","shell.execute_reply":"2024-01-16T08:32:27.032039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dc1 = load('/kaggle/input/v2-enefit-pebop-eda-plotly-and-modelling/voting_regressor_consumption_model.joblib')\ndp1 = load('/kaggle/input/v2-enefit-pebop-eda-plotly-and-modelling/voting_regressor_production_model.joblib')\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:32:27.03497Z","iopub.execute_input":"2024-01-16T08:32:27.035667Z","iopub.status.idle":"2024-01-16T08:33:20.335925Z","shell.execute_reply.started":"2024-01-16T08:32:27.035634Z","shell.execute_reply":"2024-01-16T08:33:20.334071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Declaring separate prediction functions for both models","metadata":{}},{"cell_type":"code","source":"def predict(df_features,model_consumption=c1,model_production=p1):\n    predictions = np.zeros(len(df_features))\n\n    mask = df_features[\"is_consumption\"] == 1\n    predictions[mask.values] = model_consumption.predict(\n            df_features[mask]\n    ).clip(0)\n\n    mask = df_features[\"is_consumption\"] == 0\n    predictions[mask.values] = model_production.predict(\n            df_features[mask]\n    ).clip(0)\n\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:33:20.33971Z","iopub.execute_input":"2024-01-16T08:33:20.340021Z","iopub.status.idle":"2024-01-16T08:33:20.350762Z","shell.execute_reply.started":"2024-01-16T08:33:20.339993Z","shell.execute_reply":"2024-01-16T08:33:20.34937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_model(df_features,hours_lag=48,model_consumption=dc1,model_production=dp1):\n    predictions = np.zeros(len(df_features))\n\n    mask = df_features[\"is_consumption\"] == 1\n    predictions[mask.values] = np.clip(\n        df_features[mask][f\"target_{hours_lag}h\"].fillna(0).values+ \n        model_consumption.predict(df_features[mask]),0,np.inf,\n        )\n\n    mask = df_features[\"is_consumption\"] == 0\n    predictions[mask.values] = np.clip(\n        df_features[mask][f\"target_{hours_lag}h\"].fillna(0).values+ \n        model_production.predict(df_features[mask]),0,np.inf,\n        )\n\n    return predictions\n","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:33:20.354731Z","iopub.execute_input":"2024-01-16T08:33:20.355065Z","iopub.status.idle":"2024-01-16T08:33:20.373062Z","shell.execute_reply.started":"2024-01-16T08:33:20.355036Z","shell.execute_reply":"2024-01-16T08:33:20.371059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit API","metadata":{}},{"cell_type":"code","source":"import enefit\n\nenv = enefit.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:33:20.375132Z","iopub.execute_input":"2024-01-16T08:33:20.375577Z","iopub.status.idle":"2024-01-16T08:33:20.397256Z","shell.execute_reply.started":"2024-01-16T08:33:20.375539Z","shell.execute_reply":"2024-01-16T08:33:20.395285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (\n    df_test, \n    df_new_target, \n    df_new_client, \n    df_new_historical_weather,\n    df_new_forecast_weather, \n    df_new_electricity_prices, \n    df_new_gas_prices, \n    df_sample_prediction\n) in iter_test:\n\n    data_storage.update_with_new_data(\n        df_new_client=df_new_client,\n        df_new_gas_prices=df_new_gas_prices,\n        df_new_electricity_prices=df_new_electricity_prices,\n        df_new_forecast_weather=df_new_forecast_weather,\n        df_new_historical_weather=df_new_historical_weather,\n        df_new_target=df_new_target\n    )\n    \n    #separately generate test features for both models\n    \n    df_test = data_storage.preprocess_test(df_test)\n    \n    df_test_features = features_generator.generate_features(df_test)\n    \n    df_test_feats = feat_gen.generate_features(df_test,False)\n    \n    df_test_feats.drop(columns=['date','literal'],inplace=True)\n        \n    pred1 = predict(df_test_features)\n    \n    pred2 = predict_model(df_test_feats)\n    \n    # Ensembling with slightly tuned model weights\n    df_sample_prediction[\"target\"] = (\n        (0.49 * pred1) + \n        (0.51 * pred2)\n    )\n    \n    env.predict(df_sample_prediction)\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T08:36:10.575249Z","iopub.execute_input":"2024-01-16T08:36:10.575633Z","iopub.status.idle":"2024-01-16T08:36:10.583631Z","shell.execute_reply.started":"2024-01-16T08:36:10.5756Z","shell.execute_reply":"2024-01-16T08:36:10.58277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}