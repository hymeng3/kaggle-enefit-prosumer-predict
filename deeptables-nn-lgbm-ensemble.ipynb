{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":57236,"databundleVersionId":7292407,"sourceType":"competition"},{"sourceId":160761304,"sourceType":"kernelVersion"}],"dockerImageVersionId":30588,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Based on https://www.kaggle.com/code/vitalykudelya/enefit-target-diff.\n\nDeepTables: Deep-learning Toolkit for Tabular data\n\nhttps://github.com/DataCanvasIO/DeepTables\n\nhttps://deeptables.readthedocs.io/en/latest/model_config.html#parameters","metadata":{}},{"cell_type":"code","source":"!pip install --no-index -U --find-links=/kaggle/input/deeptables-dependecies deeptables==0.2.5","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport gc\nimport math\nimport ctypes\nimport pickle\nimport random\nimport datetime\nimport holidays\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf, deeptables as dt\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow_addons.optimizers import AdamW\nfrom tensorflow.python.keras import backend as K\nfrom deeptables.models import DeepTable, ModelConfig\nfrom deeptables.models import deepnets\n\nprint('TensorFlow version:',tf.__version__)\nprint('DeepTables version:',dt.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 42\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nseed_everything(seed=seed)\n\ndef clean_memory():\n    gc.collect()\n    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\nclean_memory()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classes","metadata":{}},{"cell_type":"markdown","source":"### DataStorage","metadata":{}},{"cell_type":"code","source":"class DataStorage:\n    root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n\n    data_cols = [\n        \"target\",\n        \"county\",\n        \"is_business\",\n        \"product_type\",\n        \"is_consumption\",\n        \"datetime\",\n        \"row_id\",\n    ]\n    client_cols = [\n        \"product_type\",\n        \"county\",\n        \"eic_count\",\n        \"installed_capacity\",\n        \"is_business\",\n        \"date\",\n    ]\n    gas_prices_cols = [\"forecast_date\", \"lowest_price_per_mwh\", \"highest_price_per_mwh\"]\n    electricity_prices_cols = [\"forecast_date\", \"euros_per_mwh\"]\n    forecast_weather_cols = [\n        \"latitude\",\n        \"longitude\",\n        \"hours_ahead\",\n        \"temperature\",\n        \"dewpoint\",\n        \"cloudcover_high\",\n        \"cloudcover_low\",\n        \"cloudcover_mid\",\n        \"cloudcover_total\",\n        \"10_metre_u_wind_component\",\n        \"10_metre_v_wind_component\",\n        \"forecast_datetime\",\n        \"direct_solar_radiation\",\n        \"surface_solar_radiation_downwards\",\n        \"snowfall\",\n        \"total_precipitation\",\n    ]\n    historical_weather_cols = [\n        \"datetime\",\n        \"temperature\",\n        \"dewpoint\",\n        \"rain\",\n        \"snowfall\",\n        \"surface_pressure\",\n        \"cloudcover_total\",\n        \"cloudcover_low\",\n        \"cloudcover_mid\",\n        \"cloudcover_high\",\n        \"windspeed_10m\",\n        \"winddirection_10m\",\n        \"shortwave_radiation\",\n        \"direct_solar_radiation\",\n        \"diffuse_radiation\",\n        \"latitude\",\n        \"longitude\",\n    ]\n    location_cols = [\"longitude\", \"latitude\", \"county\"]\n    target_cols = [\n        \"target\",\n        \"county\",\n        \"is_business\",\n        \"product_type\",\n        \"is_consumption\",\n        \"datetime\",\n    ]\n\n    def __init__(self):\n        self.df_data = pl.read_csv(\n            os.path.join(self.root, \"train.csv\"),\n            columns=self.data_cols,\n            try_parse_dates=True,\n        )\n        self.df_client = pl.read_csv(\n            os.path.join(self.root, \"client.csv\"),\n            columns=self.client_cols,\n            try_parse_dates=True,\n        )\n        self.df_gas_prices = pl.read_csv(\n            os.path.join(self.root, \"gas_prices.csv\"),\n            columns=self.gas_prices_cols,\n            try_parse_dates=True,\n        )\n        self.df_electricity_prices = pl.read_csv(\n            os.path.join(self.root, \"electricity_prices.csv\"),\n            columns=self.electricity_prices_cols,\n            try_parse_dates=True,\n        )\n        self.df_forecast_weather = pl.read_csv(\n            os.path.join(self.root, \"forecast_weather.csv\"),\n            columns=self.forecast_weather_cols,\n            try_parse_dates=True,\n        )\n        self.df_historical_weather = pl.read_csv(\n            os.path.join(self.root, \"historical_weather.csv\"),\n            columns=self.historical_weather_cols,\n            try_parse_dates=True,\n        )\n        self.df_weather_station_to_county_mapping = pl.read_csv(\n            os.path.join(self.root, \"weather_station_to_county_mapping.csv\"),\n            columns=self.location_cols,\n            try_parse_dates=True,\n        )\n        self.df_data = self.df_data.filter(\n            pl.col(\"datetime\") >= pd.to_datetime(\"2022-01-01\")\n        )\n        self.df_target = self.df_data.select(self.target_cols)\n\n        self.schema_data = self.df_data.schema\n        self.schema_client = self.df_client.schema\n        self.schema_gas_prices = self.df_gas_prices.schema\n        self.schema_electricity_prices = self.df_electricity_prices.schema\n        self.schema_forecast_weather = self.df_forecast_weather.schema\n        self.schema_historical_weather = self.df_historical_weather.schema\n        self.schema_target = self.df_target.schema\n\n        self.df_weather_station_to_county_mapping = (\n            self.df_weather_station_to_county_mapping.with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            )\n        )\n\n    def update_with_new_data(\n        self,\n        df_new_client,\n        df_new_gas_prices,\n        df_new_electricity_prices,\n        df_new_forecast_weather,\n        df_new_historical_weather,\n        df_new_target,\n    ):\n        df_new_client = pl.from_pandas(\n            df_new_client[self.client_cols], schema_overrides=self.schema_client\n        )\n        df_new_gas_prices = pl.from_pandas(\n            df_new_gas_prices[self.gas_prices_cols],\n            schema_overrides=self.schema_gas_prices,\n        )\n        df_new_electricity_prices = pl.from_pandas(\n            df_new_electricity_prices[self.electricity_prices_cols],\n            schema_overrides=self.schema_electricity_prices,\n        )\n        df_new_forecast_weather = pl.from_pandas(\n            df_new_forecast_weather[self.forecast_weather_cols],\n            schema_overrides=self.schema_forecast_weather,\n        )\n        df_new_historical_weather = pl.from_pandas(\n            df_new_historical_weather[self.historical_weather_cols],\n            schema_overrides=self.schema_historical_weather,\n        )\n        df_new_target = pl.from_pandas(\n            df_new_target[self.target_cols], schema_overrides=self.schema_target\n        )\n\n        self.df_client = pl.concat([self.df_client, df_new_client]).unique(\n            [\"date\", \"county\", \"is_business\", \"product_type\"]\n        )\n        self.df_gas_prices = pl.concat([self.df_gas_prices, df_new_gas_prices]).unique(\n            [\"forecast_date\"]\n        )\n        self.df_electricity_prices = pl.concat(\n            [self.df_electricity_prices, df_new_electricity_prices]\n        ).unique([\"forecast_date\"])\n        self.df_forecast_weather = pl.concat(\n            [self.df_forecast_weather, df_new_forecast_weather]\n        ).unique([\"forecast_datetime\", \"latitude\", \"longitude\", \"hours_ahead\"])\n        self.df_historical_weather = pl.concat(\n            [self.df_historical_weather, df_new_historical_weather]\n        ).unique([\"datetime\", \"latitude\", \"longitude\"])\n        self.df_target = pl.concat([self.df_target, df_new_target]).unique(\n            [\"datetime\", \"county\", \"is_business\", \"product_type\", \"is_consumption\"]\n        )\n\n    def preprocess_test(self, df_test):\n        df_test = df_test.rename(columns={\"prediction_datetime\": \"datetime\"})\n        df_test = pl.from_pandas(\n            df_test[self.data_cols[1:]], schema_overrides=self.schema_data\n        )\n        return df_test","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### FeaturesGenerator","metadata":{}},{"cell_type":"code","source":"class FeaturesGenerator:\n    def __init__(self, data_storage):\n        self.data_storage = data_storage\n        self.estonian_holidays = list(\n            holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n        )\n\n    def _add_general_features(self, df_features):\n        df_features = (\n            df_features.with_columns(\n                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n                pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n                pl.col(\"datetime\").dt.day().alias(\"day\"),\n                pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n                pl.col(\"datetime\").dt.month().alias(\"month\"),\n                pl.col(\"datetime\").dt.year().alias(\"year\"),\n            )\n            .with_columns(\n                pl.concat_str(\n                    \"county\",\n                    \"is_business\",\n                    \"product_type\",\n                    \"is_consumption\",\n                    separator=\"_\",\n                ).alias(\"segment\"),\n            )\n            .with_columns(\n                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n                (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n                (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n            )\n        )\n        return df_features\n\n    def _add_client_features(self, df_features):\n        df_client = self.data_storage.df_client\n\n        df_features = df_features.join(\n            df_client.with_columns(\n                (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n            ),\n            on=[\"county\", \"is_business\", \"product_type\", \"date\"],\n            how=\"left\",\n        )\n        return df_features\n    \n    def is_country_holiday(self, row):\n        return (\n            datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])\n            in self.estonian_holidays\n        )\n\n    def _add_holidays_features(self, df_features):\n        df_features = df_features.with_columns(\n            pl.struct([\"year\", \"month\", \"day\"])\n            .apply(self.is_country_holiday)\n            .alias(\"is_country_holiday\")\n        )\n        return df_features\n\n    def _add_forecast_weather_features(self, df_features):\n        df_forecast_weather = self.data_storage.df_forecast_weather\n        df_weather_station_to_county_mapping = (\n            self.data_storage.df_weather_station_to_county_mapping\n        )\n\n        df_forecast_weather = (\n            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n            .drop(\"hours_ahead\")\n            .with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            )\n            .join(\n                df_weather_station_to_county_mapping,\n                how=\"left\",\n                on=[\"longitude\", \"latitude\"],\n            )\n            .drop(\"longitude\", \"latitude\")\n        )\n\n        df_forecast_weather_date = (\n            df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\")\n        )\n\n        df_forecast_weather_local = (\n            df_forecast_weather.filter(pl.col(\"county\").is_not_null())\n            .group_by(\"county\", \"datetime\")\n            .mean()\n        )\n\n        for hours_lag in [0, 7 * 24]:\n            df_features = df_features.join(\n                df_forecast_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_forecast_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_forecast_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=[\"county\", \"datetime\"],\n                how=\"left\",\n                suffix=f\"_forecast_local_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _add_historical_weather_features(self, df_features):\n        df_historical_weather = self.data_storage.df_historical_weather\n        df_weather_station_to_county_mapping = (\n            self.data_storage.df_weather_station_to_county_mapping\n        )\n\n        df_historical_weather = (\n            df_historical_weather.with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            )\n            .join(\n                df_weather_station_to_county_mapping,\n                how=\"left\",\n                on=[\"longitude\", \"latitude\"],\n            )\n            .drop(\"longitude\", \"latitude\")\n        )\n\n        df_historical_weather_date = (\n            df_historical_weather.group_by(\"datetime\").mean().drop(\"county\")\n        )\n\n        df_historical_weather_local = (\n            df_historical_weather.filter(pl.col(\"county\").is_not_null())\n            .group_by(\"county\", \"datetime\")\n            .mean()\n        )\n\n        for hours_lag in [2 * 24, 7 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_historical_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_historical_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=[\"county\", \"datetime\"],\n                how=\"left\",\n                suffix=f\"_historical_local_{hours_lag}h\",\n            )\n\n        for hours_lag in [1 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n                )\n                .filter(pl.col(\"hour\") <= 10)\n                .drop(\"hour\"),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_historical_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _add_target_features(self, df_features):\n        df_target = self.data_storage.df_target\n\n        df_target_all_type_sum = (\n            df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"])\n            .sum()\n            .drop(\"product_type\")\n        )\n\n        df_target_all_county_type_sum = (\n            df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"])\n            .sum()\n            .drop(\"product_type\", \"county\")\n        )\n\n        for hours_lag in [\n            2 * 24,\n            3 * 24,\n            4 * 24,\n            5 * 24,\n            6 * 24,\n            7 * 24,\n            8 * 24,\n            9 * 24,\n            10 * 24,\n            11 * 24,\n            12 * 24,\n            13 * 24,\n            14 * 24,\n        ]:\n            df_features = df_features.join(\n                df_target.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n                on=[\n                    \"county\",\n                    \"is_business\",\n                    \"product_type\",\n                    \"is_consumption\",\n                    \"datetime\",\n                ],\n                how=\"left\",\n            )\n\n        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n            df_features = df_features.join(\n                df_target_all_type_sum.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],\n                how=\"left\",\n            )\n\n            df_features = df_features.join(\n                df_target_all_county_type_sum.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n                on=[\"is_business\", \"is_consumption\", \"datetime\"],\n                how=\"left\",\n                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n            )\n\n        cols_for_stats = [\n            f\"target_{hours_lag}h\" for hours_lag in [2 * 24, 3 * 24, 4 * 24, 5 * 24]\n        ]\n        df_features = df_features.with_columns(\n            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n            df_features.select(cols_for_stats)\n            .transpose()\n            .std()\n            .transpose()\n            .to_series()\n            .alias(f\"target_std\"),\n        )\n\n        for target_prefix, lag_nominator, lag_denomonator in [\n            (\"target\", 24 * 7, 24 * 14),\n            (\"target\", 24 * 2, 24 * 9),\n            (\"target\", 24 * 3, 24 * 10),\n            (\"target\", 24 * 2, 24 * 3),\n            (\"target_all_type_sum\", 24 * 2, 24 * 3),\n            (\"target_all_type_sum\", 24 * 7, 24 * 14),\n            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),\n            (\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n        ]:\n            df_features = df_features.with_columns(\n                (\n                    pl.col(f\"{target_prefix}_{lag_nominator}h\")\n                    / (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n            )\n\n        return df_features\n\n    def _reduce_memory_usage(self, df_features):\n        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n        return df_features\n\n    def _drop_columns(self, df_features):\n        df_features = df_features.drop(\n            \"date\", \"datetime\", \"hour\", \"dayofyear\"\n        )\n        return df_features\n\n    def _to_pandas(self, df_features, y):\n        cat_cols = [\n            \"county\",\n            \"is_business\",\n            \"product_type\",\n            \"is_consumption\",\n            \"segment\",\n        ]\n\n        if y is not None:\n            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n        else:\n            df_features = df_features.to_pandas()\n\n        df_features = df_features.set_index(\"row_id\")\n        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n\n        return df_features\n\n    def generate_features(self, df_prediction_items):\n        if \"target\" in df_prediction_items.columns:\n            df_prediction_items, y = (\n                df_prediction_items.drop(\"target\"),\n                df_prediction_items.select(\"target\"),\n            )\n        else:\n            y = None\n\n        df_features = df_prediction_items.with_columns(\n            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n        )\n\n        for add_features in [\n            self._add_general_features,\n            self._add_client_features,\n            self._add_forecast_weather_features,\n            self._add_historical_weather_features,\n            self._add_target_features,\n            self._add_holidays_features,\n            self._reduce_memory_usage,\n            self._drop_columns,\n        ]:\n            df_features = add_features(df_features)\n\n        df_features = self._to_pandas(df_features, y)\n\n        return df_features","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Generation","metadata":{}},{"cell_type":"code","source":"%%time\ndata_storage = DataStorage()\nfeatures_generator = FeaturesGenerator(data_storage=data_storage)\n\ndf_train_features = features_generator.generate_features(data_storage.df_data)\ndf_train_features = df_train_features[df_train_features['target'].notnull()]\n\nclean_memory()\ndf_train_features.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"markdown","source":"### LearningRateScheduler","metadata":{}},{"cell_type":"code","source":"LR_START = 1e-7\nLR_MAX = 1e-3\nLR_MIN = 1e-7\nLR_RAMPUP_EPOCHS = 2\nLR_SUSTAIN_EPOCHS = 2\nEPOCHS = 10\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        decay_total_epochs = EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n        decay_epoch_index = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n        phase = math.pi * decay_epoch_index / decay_total_epochs\n        cosine_decay = 0.5 * (1 + math.cos(phase))\n        lr = (LR_MAX - LR_MIN) * cosine_decay + LR_MIN\n        \n    return lr\n\nrng = [i for i in range(EPOCHS)]\nlr_y = [lrfn(x) for x in rng]\nplt.figure(figsize=(10, 4))\nplt.plot(rng, lr_y, '-o')\nplt.xlabel('Epoch'); plt.ylabel('LR')\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\". \\\n      format(lr_y[0], max(lr_y), lr_y[-1]))\nLR = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"%%time\nclass CFG:\n    nn = True\n    lgb = True\n    ens_weights = {'nn': 0.5, 'lgb': 0.5}\n    epochs = 10\n    batch_size = 512\n    valid_size = 5e-2\n    LR_Scheduler = []  # [LR]\n    optimizer = AdamW(learning_rate=1e-3, weight_decay=9e-7)\n     \nclass Model:\n    def __init__(self):\n        self.conf = ModelConfig(auto_imputation=True,\n                                auto_discrete=True,\n                                auto_discard_unique=True,\n                                categorical_columns='auto',\n                                fixed_embedding_dim=False,\n                                embeddings_output_dim=4,\n                                embedding_dropout=0.3,\n                                nets=['dnn_nets'],\n                                dnn_params={\n                                    'hidden_units': ((512, 0.3, True),\n                                                     (256, 0.3, True)),\n                                    'dnn_activation': 'relu',\n                                },\n                                stacking_op='add',\n                                output_use_bias=False,\n                                optimizer=CFG.optimizer,\n                                task='regression',\n                                loss='MeanAbsoluteError',\n                                metrics='MeanAbsoluteError',\n                                earlystopping_patience=1,\n                                )\n        \n        self.lgb_params = {\"n_estimators\": 2500,\n                           \"learning_rate\": 0.06,\n                           \"max_depth\": 16,\n                           \"num_leaves\": 500,\n                           \"reg_alpha\": 3.5,\n                           \"reg_lambda\": 1.5,\n                           \"colsample_bytree\": 0.9,\n                           \"colsample_bynode\": 0.6,\n                           \"min_child_samples\": 50,\n                           \"random_state\": 0,\n                           \"objective\": \"regression_l1\",\n                           \"device\": \"gpu\",\n                           \"n_jobs\": 4,\n                           \"verbose\": -1,\n                           }\n        \n        self.nn_model_consumption = DeepTable(config=self.conf)  \n        self.nn_model_production = DeepTable(config=self.conf)\n        \n        self.lgb_model_consumption = lgb.LGBMRegressor(**self.lgb_params)\n        self.lgb_model_production = lgb.LGBMRegressor(**self.lgb_params)\n\n    def fit(self, df_train_features):\n        print('nn = '+str(CFG.nn))\n        print('lgb = '+str(CFG.lgb))\n        \n        if CFG.nn == True:\n            \n            print('\\n',\"nn model consumption training.\",'\\n')\n            mask = df_train_features[\"is_consumption\"] == 1\n            self.nn_model_consumption.fit(\n                X=df_train_features[mask].drop(columns=[\"target\"]),\n                y=df_train_features[mask][\"target\"]\n                - df_train_features[mask][\"target_48h\"].fillna(0),\n                validation_split=CFG.valid_size, shuffle=False,\n                batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2,\n                callbacks=CFG.LR_Scheduler\n            )\n        \n            # Avoid saving error\n            with K.name_scope(CFG.optimizer.__class__.__name__):\n                for i, var in enumerate(CFG.optimizer.weights):\n                    name = 'variable{}'.format(i)\n                    CFG.optimizer.weights[i] = tf.Variable(var, name=name)\n            self.conf = self.conf._replace(optimizer=CFG.optimizer)   \n            self.nn_model_production = DeepTable(config=self.conf)\n            \n            print('\\n',\"nn model production training.\",'\\n')\n            mask = df_train_features[\"is_consumption\"] == 0\n            self.nn_model_production.fit(\n                X=df_train_features[mask].drop(columns=[\"target\"]),\n                y=df_train_features[mask][\"target\"]\n                - df_train_features[mask][\"target_48h\"].fillna(0),\n                validation_split=CFG.valid_size, shuffle=False,\n                batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2,\n                callbacks=CFG.LR_Scheduler\n            )\n        \n        if CFG.lgb == True:\n            \n            print('\\n',\"lgb model consumption training.\")\n            mask = df_train_features[\"is_consumption\"] == 1\n            self.lgb_model_consumption.fit(\n                X=df_train_features[mask].drop(columns=[\"target\"]),\n                y=df_train_features[mask][\"target\"]\n                - df_train_features[mask][\"target_48h\"].fillna(0),\n            )\n        \n            print('\\n',\"lgb model production training.\",'\\n')\n            mask = df_train_features[\"is_consumption\"] == 0\n            self.lgb_model_production.fit(\n                X=df_train_features[mask].drop(columns=[\"target\"]),\n                y=df_train_features[mask][\"target\"]\n                - df_train_features[mask][\"target_48h\"].fillna(0),\n            )\n        \n    def plot_nn_model(self):\n        if CFG.nn == True:\n            return plot_model(self.nn_model_consumption.get_model().model)    \n\n    def predict(self, df_features):\n        predictions = np.zeros(len(df_features))\n        \n        if CFG.nn == True and CFG.lgb == True:\n            \n            print('\\n',\"nn & lgb model consumption prediction.\",'\\n')\n            mask = df_features[\"is_consumption\"] == 1\n            predictions[mask.values] = np.clip(\n                df_features[mask][\"target_48h\"].fillna(0).values\n                + CFG.ens_weights['nn'] * (self.nn_model_consumption.predict(df_features[mask])[:,0])\n                + CFG.ens_weights['lgb'] * (self.lgb_model_consumption.predict(df_features[mask])),\n                0,\n                np.inf,\n            )\n        \n            print('\\n',\"nn & lgb model production prediction.\",'\\n')\n            mask = df_features[\"is_consumption\"] == 0\n            predictions[mask.values] = np.clip(\n                df_features[mask][\"target_48h\"].fillna(0).values\n                + CFG.ens_weights['nn'] * (self.nn_model_production.predict(df_features[mask])[:,0])\n                + CFG.ens_weights['lgb'] * (self.lgb_model_production.predict(df_features[mask])),\n                0,\n                np.inf,\n            )\n        \n        elif CFG.nn == True and CFG.lgb == False:\n            \n            print('\\n',\"nn model consumption prediction.\",'\\n')\n            mask = df_features[\"is_consumption\"] == 1\n            predictions[mask.values] = np.clip(\n                df_features[mask][\"target_48h\"].fillna(0).values\n                + self.nn_model_consumption.predict(df_features[mask])[:,0],\n                0,\n                np.inf,\n            )\n            \n            print('\\n',\"nn model production prediction.\",'\\n')\n            mask = df_features[\"is_consumption\"] == 0\n            predictions[mask.values] = np.clip(\n                df_features[mask][\"target_48h\"].fillna(0).values\n                + self.nn_model_production.predict(df_features[mask])[:,0],\n                0,\n                np.inf,\n            )\n            \n        elif CFG.nn == False and CFG.lgb == True:\n            \n            print('\\n',\"lgb model consumption prediction.\",'\\n')\n            mask = df_features[\"is_consumption\"] == 1\n            predictions[mask.values] = np.clip(\n                df_features[mask][\"target_48h\"].fillna(0).values\n                + self.lgb_model_consumption.predict(df_features[mask]),\n                0,\n                np.inf,\n            )\n            \n            print('\\n',\"lgb model production prediction.\",'\\n')\n            mask = df_features[\"is_consumption\"] == 0\n            predictions[mask.values] = np.clip(\n                df_features[mask][\"target_48h\"].fillna(0).values\n                + self.lgb_model_production.predict(df_features[mask]),\n                0,\n                np.inf,\n            )\n            \n        else:\n            raise ValueError(\"No models has been trained.\")\n            \n        return predictions\n    \n    \nmodel = Model()\nmodel.fit(df_train_features)\nclean_memory()\nmodel.plot_nn_model()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit API","metadata":{}},{"cell_type":"code","source":"import enefit\n\nenv = enefit.make_env()\niter_test = env.iter_test()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfor (\n    df_test, \n    df_new_target, \n    df_new_client, \n    df_new_historical_weather,\n    df_new_forecast_weather, \n    df_new_electricity_prices, \n    df_new_gas_prices, \n    df_sample_prediction\n) in iter_test:\n\n    data_storage.update_with_new_data(\n        df_new_client=df_new_client,\n        df_new_gas_prices=df_new_gas_prices,\n        df_new_electricity_prices=df_new_electricity_prices,\n        df_new_forecast_weather=df_new_forecast_weather,\n        df_new_historical_weather=df_new_historical_weather,\n        df_new_target=df_new_target\n    )\n    df_test = data_storage.preprocess_test(df_test)\n    \n    df_test_features = features_generator.generate_features(df_test)\n    df_sample_prediction[\"target\"] = model.predict(df_test_features)\n    \n    env.predict(df_sample_prediction)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}